{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "1. Base Model is Chosen\n",
    "2. Layers are extracted using get_layer(name).output\n",
    "3. The Dream model is built off the inputs and outputs of the model\n",
    "4. Tiled Gradients is fed the Dream model\n",
    "5. Deep Dream with Octaves is run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Base Model is Chosen\n",
    "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Layers are extracted using get_layer(name).output\n",
    "layers = [layer.name for layer in base_model.layers if 'mixed' in layer.name]\n",
    "layers = [base_model.get_layer(name).output for name in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mixed0/concat:0' shape=(None, None, None, 256) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.get_layer('mixed0').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. The Dream model is built off the inputs and outputs of the model\n",
    "dream_model = tf.keras.Model(inputs=base_model.input, outputs=[layers[3]])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Loss\n",
    "\n",
    "The loss is the sum of the activations in the chosen layers. The loss is normalized at each layer so the contribution from larger layers does not outweigh smaller layers. Normally, loss is a quantity you wish to minimize via gradient descent. In DeepDream, you will maximize this loss via gradient ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.randint(0, 255, (500,500,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(img, model):\n",
    "    img_batch = tf.expand_dims(img, axis = 0) ## batch size of 1\n",
    "    layer_activations = model(img_batch)\n",
    "    if len(layer_activations) == 1:\n",
    "        layer_activations = [layer_activations]\n",
    "    \n",
    "    losses = []\n",
    "    for act in layer_activations:\n",
    "        loss = tf.math.reduce_mean(act)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    return tf.reduce_sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDream(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=(\n",
    "            tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=[], dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=[], dtype=tf.float32),)\n",
    "                  )\n",
    "    \n",
    "    def __call__(self, img, steps, step_size):\n",
    "        print(\"Tracing\")\n",
    "        loss = tf.constant(0.0)\n",
    "        for n in tf.range(steps):\n",
    "            with tf.GradientTape() as tape:\n",
    "              # This needs gradients relative to `img`\n",
    "              # `GradientTape` only watches `tf.Variable`s by default\n",
    "                tape.watch(img)\n",
    "                loss = calc_loss(img, self.model)\n",
    "\n",
    "            # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
    "            gradients = tape.gradient(loss, img)\n",
    "\n",
    "            # Normalize the gradients.\n",
    "            gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
    "\n",
    "            # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
    "            # You can update the image by directly adding the gradients (because they're the same shape!)\n",
    "            img = img + gradients*step_size\n",
    "            img = tf.clip_by_value(img, -1, 1)\n",
    "\n",
    "        return loss, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
